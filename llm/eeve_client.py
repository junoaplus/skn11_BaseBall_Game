# llm/eeve_client.py

from ollama import Client

def get_decision_from_llm(prompt: str) -> str:
    try:
        client = Client(host='http://localhost:11434')  # Ollama ì„œë²„ ê¸°ë³¸ í¬íŠ¸
        response = client.chat(model='llama3.2', messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì•¼êµ¬ ì‘ì „ ì½”ì¹˜ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ])
        return response['message']['content'].strip()
    except Exception as e:
        print("ğŸ›‘ LLM ì‹¤í–‰ ì‹¤íŒ¨:", e)
        return "ê°•ê³µì„ ì„ íƒí•©ë‹ˆë‹¤. ê¸°ë³¸ íŒë‹¨ì…ë‹ˆë‹¤."
